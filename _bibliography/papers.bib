---
---
@inproceedings{le2022inftybrush,
  title={$\infty$-Brush: Controllable Large Image Synthesis with Diffusion Models in Infinite Dimensions},
  author={Le*, Minh-Quan and Graikos*, Alexandros and Yellapragada, Srikar and Gupta, Rajarsi and Saltz, Joel and Samaras, Dimitris},
  abbr={ECCV},
  booktitle={European Conference on Computer Vision},
  year={2024},
  organization={Springer},
  selected={true},
  preview={eccv24.pdf},
  abstract={Synthesizing high-resolution images from intricate, domain-specific information remains a significant challenge in generative modeling, particularly for applications in large-image domains such as digital histopathology and remote sensing. Existing methods face critical limitations: conditional diffusion models in pixel or latent space cannot exceed the resolution on which they were trained without losing fidelity, and computational demands increase significantly for larger image sizes. Patch-based methods offer computational efficiency but fail to capture long-range spatial relationships due to their overreliance on local information. In this paper, we introduce a novel conditional diffusion model in infinite dimensions, $\infty$-Brush for controllable large image synthesis. We propose a cross-attention neural operator to enable conditioning in function space. Our model overcomes the constraints of traditional finite-dimensional diffusion models and patch-based methods, offering scalability and superior capability in preserving global image structures while maintaining fine details. To our best knowledge, $\infty$-Brush is the first conditional diffusion model in function space, that can controllably synthesize images at arbitrary resolutions of up to 4096 x 4096 pixels. The code is available at https://github.com/cvlab-stonybrook/infinity-brush.},
  pdf={eccv24.pdf},
}

@inproceedings{le2024maskdiff,
  title={MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation},
  author={Le, Minh-Quan and Nguyen, Tam V and Le, Trung-Nghia and Do, Thanh-Toan and Do, Minh N and Tran, Minh-Triet},
  abbr={AAAI},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={3},
  pages={2874--2881},
  year={2024},
  selected={true},
  award={Oral Presentation},
  preview={aaai24.pdf},
  pdf={https://ojs.aaai.org/index.php/AAAI/article/view/28068},
  abstract={Few-shot instance segmentation extends the few-shot learning paradigm to the instance segmentation task, which tries to segment instance objects from a query image with a few annotated examples of novel categories. Conventional approaches have attempted to address the task via prototype learning, known as point estimation. However, this mechanism depends on prototypes (e.g. mean of K-shot) for prediction, leading to performance instability. To overcome the disadvantage of the point estimation mechanism, we propose a novel approach, dubbed MaskDiff, which models the underlying conditional distribution of a binary mask, which is conditioned on an object region and K-shot information. Inspired by augmentation approaches that perturb data with Gaussian noise for populating low data density regions, we model the mask distribution with a diffusion probabilistic model. We also propose to utilize classifier-free guided mask sampling to integrate category information into the binary mask generation process. Without bells and whistles, our proposed method consistently outperforms state-of-the-art methods on both base and novel classes of the COCO dataset while simultaneously being more stable than existing methods. The source code is available at: https://github.com/minhquanlecs/MaskDiff.}

}

@inproceedings{graikos2024learned,
  title={Learned representation-guided diffusion models for large-image generation},
  abbr={CVPR},
  author={Graikos*, Alexandros and Yellapragada*, Srikar and Le, Minh-Quan and Kapse, Saarthak and Prasanna, Prateek and Saltz, Joel and Samaras, Dimitris},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8532--8542},
  year={2024},
  selected={true},
  preview={cvpr24.pdf},
  pdf={https://openaccess.thecvf.com/content/CVPR2024/papers/Graikos_Learned_Representation-Guided_Diffusion_Models_for_Large-Image_Generation_CVPR_2024_paper.pdf},
  abstract={To synthesize high-fidelity samples, diffusion models typically require auxiliary data to guide the generation process. However, it is impractical to procure the painstaking patch-level annotation effort required in specialized domains like histopathology and satellite imagery; it is often performed by domain experts and involves hundreds of millions of patches. Modern-day self-supervised learning (SSL) representations encode rich semantic and visual information. In this paper, we posit that such representations are expressive enough to act as proxies to fine-grained human labels. We introduce a novel approach that trains diffusion models conditioned on embeddings from SSL. Our diffusion models successfully project these features back to high-quality histopathology and remote sensing images. In addition, we construct larger images by assembling spatially consistent patches inferred from SSL embeddings, preserving long-range dependencies. Augmenting real data by generating variations of real images improves downstream classifier accuracy for patch-level and larger, image-scale classification tasks. Our models are effective even on datasets not encountered during training, demonstrating their robustness and generalizability. Generating images from learned embeddings is agnostic to the source of the embeddings. The SSL embeddings used to generate a large image can either be extracted from a reference image, or sampled from an auxiliary model conditioned on any related modality (e.g. class labels, text, genomic data). As proof of concept, we introduce the text-to-large image synthesis paradigm where we successfully synthesize large pathology and satellite images out of text descriptions.}
}

@article{le2023sketchanimar,
  title={SketchANIMAR: sketch-based 3D animal fine-grained retrieval},
  author={Le, Trung-Nghia and Nguyen, Tam V and Le, Minh-Quan and Nguyen, Trong-Thuan and Huynh, Viet-Tham and Do, Trong-Le and Le, Khanh-Duy and Tran, Mai-Khiem and Hoang-Xuan, Nhat and Nguyen-Ho, Thang-Long and others},
  journal={Computers \& Graphics},
  volume={116},
  pages={150--161},
  year={2023},
  publisher={Elsevier},
  abbr={CG}
}

@article{le2023textanimar,
  title={TextANIMAR: text-based 3D animal fine-grained retrieval},
  author={Le, Trung-Nghia and Nguyen, Tam V and Le, Minh-Quan and Nguyen, Trong-Thuan and Huynh, Viet-Tham and Do, Trong-Le and Le, Khanh-Duy and Tran, Mai-Khiem and Hoang-Xuan, Nhat and Nguyen-Ho, Thang-Long and others},
  journal={Computers \& Graphics},
  volume={116},
  pages={162--172},
  year={2023},
  publisher={Elsevier},
  abbr={CG},
}

@article{le2023unveiling,
  title={Unveiling camouflage: A learnable Fourier-based augmentation for camouflaged object detection and instance segmentation},
  author={Le, Minh-Quan and Tran, Minh-Triet and Le, Trung-Nghia and Nguyen, Tam V and Do, Thanh-Toan},
  journal={arXiv preprint arXiv:2308.15660},
  year={2023}
}

@inproceedings{le2023fl,
  title={FL-Former: Flood Level Estimation with Vision Transformer for Images from Cameras in Urban Areas},
  author={Le, Quoc-Cuong and Le, Minh-Quan and Tran, Mai-Khiem and Le, Ngoc-Quyen and Tran, Minh-Triet},
  booktitle={International Conference on Multimedia Modeling},
  pages={447--459},
  year={2023},
  organization={Springer}
}

@inproceedings{nguyen2022data,
  title={Data-Driven City Traffic Planning Simulation},
  author={Nguyen, Tam V and Tran, Thanh Ngoc-Dat and Huynh, Viet-Tham and Truong, Bao and Le, Minh-Quan and Kumavat, Mohit and Patel, Vatsa S and Tran, Mai-Khiem and Tran, Minh-Triet},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
  pages={859--864},
  year={2022},
  organization={IEEE}
}

@article{emporio2022shrec,
  title={SHREC 2022 track on online detection of heterogeneous gestures},
  author={Emporio, Marco and Caputo, Ariel and Giachetti, Andrea and Cristani, Marco and Borghi, Guido and D’Eusanio, Andrea and Le, Minh-Quan and Nguyen, Hai-Dang and Tran, Minh-Triet and Ambellan, Felix and others},
  journal={Computers \& Graphics},
  volume={107},
  pages={241--251},
  year={2022},
  publisher={Elsevier},
  abbr={CG},
}

@inproceedings{tran2022v,
  title={V-first: A flexible interactive retrieval system for video at vbs 2022},
  author={Tran, Minh-Triet and Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Le, Thanh-Cong and Tran, Mai-Khiem and Le, Minh-Quan and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal},
  booktitle={International Conference on Multimedia Modeling},
  pages={562--568},
  year={2022},
  organization={Springer}
}

@article{le2021camouflaged,
  title={Camouflaged instance segmentation in-the-wild: Dataset, method, and benchmark suite},
  author={Le, Trung-Nghia and Cao, Yubo and Nguyen, Tan-Cong and Le, Minh-Quan and Nguyen, Khanh-Duy and Do, Thanh-Toan and Tran, Minh-Triet and Nguyen, Tam V},
  journal={IEEE Transactions on Image Processing},
  volume={31},
  pages={287--300},
  year={2021},
  publisher={IEEE}
}

@article{caputo2021shrec,
  title={SHREC 2021: Skeleton-based hand gesture recognition in the wild},
  author={Caputo, Ariel and Giachetti, Andrea and Soso, Simone and Pintani, Deborah and D’Eusanio, Andrea and Pini, Stefano and Borghi, Guido and Simoni, Alessandro and Vezzani, Roberto and Cucchiara, Rita and others},
  journal={Computers \& Graphics},
  volume={99},
  pages={201--211},
  year={2021},
  publisher={Elsevier},
  abbr={CG},
}

@inproceedings{le2021interactive,
  title={Interactive video object mask annotation},
  author={Le, Trung-Nghia and Nguyen, Tam V and Tran, Quoc-Cuong and Nguyen, Lam and Hoang, Trung-Hieu and Le, Minh-Quan and Tran, Minh-Triet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={18},
  pages={16067--16070},
  year={2021},
  abbr={AAAI},
}

@article{thompson2020shrec,
  title={SHREC 2020: Retrieval of digital surfaces with similar geometric reliefs},
  author={Thompson, Elia Moscoso and Biasotti, Silvia and Giachetti, Andrea and Tortorici, Claudio and Werghi, Naoufel and Obeid, Ahmad Shaker and Berretti, Stefano and Nguyen-Dinh, Hoang-Phuc and Le, Minh-Quan and Nguyen, Hai-Dang and others},
  journal={Computers \& Graphics},
  volume={91},
  pages={199--218},
  year={2020},
  publisher={Elsevier},
  abbr={CG},
}

@inproceedings{tran2020multi,
  title={Multi-referenced guided instance segmentation framework for semi-supervised video instance segmentation},
  author={Tran, Minh-Triet and Hoang, T and Nguyen, Tam V and Le, Trung-Nghia and Nguyen, E and Le, M and Nguyen-Dinh, H and Hoang, X and Do, Minh N},
  booktitle={CVPR Workshops},
  pages={1--4},
  year={2020}
}

@inproceedings{tran2020itask,
  title={iTASK-Intelligent traffic analysis software kit},
  author={Tran, Minh-Triet and Nguyen, Tam V and Hoang, Trung-Hieu and Le, Trung-Nghia and Nguyen, Khac-Tuan and Dinh, Dat-Thanh and Nguyen, Thanh-An and Nguyen, Hai-Dang and Hoang, Xuan-Nhat and Nguyen, Trong-Tung and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={612--613},
  year={2020}
}